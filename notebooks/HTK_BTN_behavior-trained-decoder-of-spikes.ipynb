{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <a href=\"https://githubtocolab.com/neurohari/NeuralDecoders/blob/main/notebooks/behavior-trained-decoder-of-spikes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Download\n",
    "\n",
    "First, we need to install and import the packages we need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the ones you need:\n",
    "\n",
    "# nlb_tools\n",
    "#!pip install git+https://github.com/neurallatents/nlb_tools.git \n",
    "\n",
    "# PyTorch (for modeling)\n",
    "#!pip install torch\n",
    "\n",
    "# DANDI CLI tool (optional, can use website instead)\n",
    "#!pip install dandi\n",
    "\n",
    "# EvalAI-CLI (optional, can use website instead)\n",
    "# !pip install evalai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are available on the platform DANDI. They can be downloaded directly from the website or by using the DANDI CLI tool, as shown below. For this notebook, we will be using the MC_Maze_Large dataset, which is available from [here](https://dandiarchive.org/dandiset/000138). Links to the other datasets can be found on [their website](https://neurallatents.github.io/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dandi download https://dandiarchive.org/dandiset/000138"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line will download two files into the folder `./000138/sub-Jenkins/`. Next, we'll get the path of the downloaded files and list them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "curr_path = os.getcwd()\n",
    "fpath = curr_path + '/datasets/000138/sub-Jenkins/'\n",
    "os.listdir(fpath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file with 'desc-train' in its name is for training, while the file with 'desc-test' in its name is for final model evaluation. As we take a look at the data, we will see the differences between these two files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "\n",
    "dataset = NWBDataset(fpath=fpath) \n",
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.sample(5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.trial_info;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Resampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data is at 1 ms resolution, but the NLB'21 challenge expects submissions to be at 5 ms resolution, so we will resample the data before doing any other processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Data shape: {dataset.data.shape}')\n",
    "print(f'Bin width: {dataset.bin_width} ms')\n",
    "dataset.resample(5)\n",
    "print(f'Resampled data shape: {dataset.data.shape}')\n",
    "print(f'Resampled bin width: {dataset.bin_width} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.make_tensors import make_train_input_tensors, make_eval_input_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_train_input_tensors` extracts the data available for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = make_train_input_tensors(dataset=dataset, \n",
    "                                      dataset_name='mc_maze_large', \n",
    "                                      trial_split='train', # trial_split=['train', 'val'], for Test phase\n",
    "                                      save_file=False, \n",
    "                                      include_behavior=True,\n",
    "                                      include_forward_pred=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_eval_input_tensors` extracts the data used to evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = make_eval_input_tensors(dataset=dataset,\n",
    "                                    dataset_name='mc_maze_large',\n",
    "                                    trial_split='val', # trial_split='test', for Test phase\n",
    "                                    save_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dict['train_spikes_heldin'].shape)\n",
    "print(train_dict['train_spikes_heldin_forward'].shape)\n",
    "print(train_dict['train_behavior'].shape)\n",
    "print(train_dict['train_spikes_heldout'].shape)\n",
    "print(train_dict['train_spikes_heldout_forward'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dict['eval_spikes_heldin'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "We define a class that models the data with an RNN and uses an exponential mapping to firing rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NLBRNN(torch.nn.Module):\n",
    "    \"\"\"Simple RNN to model spiking data\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(NLBRNN, self).__init__()\n",
    "        self.rnn = torch.nn.GRU(input_size=input_dim,\n",
    "                                    hidden_size=hidden_dim,\n",
    "                                    num_layers=num_layers,\n",
    "                                    batch_first=True,\n",
    "                                    bidirectional=False,\n",
    "                                    **factory_kwargs)\n",
    "        self.transform = torch.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        output, hidden = self.rnn(X)\n",
    "        output = self.transform(output)\n",
    "        return torch.exp(output)\n",
    "\n",
    "class NLBRunner:\n",
    "    \"\"\"Class that handles training NLBRNN\"\"\"\n",
    "    def __init__(self, model_init, model_cfg, data, train_cfg, use_gpu=False, num_gpus=1):\n",
    "        self.model = model_init(**model_cfg)\n",
    "        self.data = data\n",
    "        self.cd_ratio = train_cfg.get('cd_ratio', 0.2)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), \n",
    "                                          lr=train_cfg.get('lr', 1e-3), \n",
    "                                          weight_decay=train_cfg.get('alpha', 0.0))\n",
    "    \n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Trains model for one epoch. \n",
    "        This simple script does not support splitting training samples into batches.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        # create mask for coordinated dropout\n",
    "        train_input, train_output, val_input, val_output, *_ = self.data\n",
    "\n",
    "        # mask inputs\n",
    "        masked_train_input = train_input.clone()\n",
    "\n",
    "        train_predictions = self.model(masked_train_input)\n",
    "        # learn only from masked inputs\n",
    "        loss = torch.nn.functional.poisson_nll_loss(train_predictions, train_output, log_input=False)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # get validation score\n",
    "        train_res, train_output = self.score(train_input, train_output, prefix='train')\n",
    "        val_res, val_output = self.score(val_input, val_output, prefix='val')\n",
    "        res = train_res.copy()\n",
    "        res.update(val_res)\n",
    "        return res, (train_output, val_output)\n",
    "    \n",
    "    def score(self, input, output, prefix='val'):\n",
    "        \"\"\"Evaluates model performance on given data\"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = self.model(input)\n",
    "        self.model.train()\n",
    "        loss = torch.nn.functional.poisson_nll_loss(predictions, output, log_input=False)\n",
    "        num_heldout = output.shape[2] - input.shape[2]\n",
    "        cosmooth_loss = torch.nn.functional.poisson_nll_loss(\n",
    "            predictions[:, :, -num_heldout:], output[:, :, -num_heldout:], log_input=False)\n",
    "        return {f'{prefix}_nll': loss.item(), f'{prefix}_cosmooth_nll': cosmooth_loss.item()}, predictions\n",
    "\n",
    "    def train(self, n_iter=1000, patience=200, save_path=None, verbose=False, log_frequency=50):\n",
    "        \"\"\"Trains model for given number of iterations with early stopping\"\"\"\n",
    "        train_log = []\n",
    "        best_score = 1e8\n",
    "        last_improv = -1\n",
    "        for i in range(n_iter):\n",
    "            res, output = self.train_epoch()\n",
    "            res['iter'] = i\n",
    "            train_log.append(res)\n",
    "            if verbose:\n",
    "                if (i % log_frequency) == 0:\n",
    "                    print(res)\n",
    "            if res['val_nll'] < best_score:\n",
    "                best_score = res['val_nll']\n",
    "                last_improv = i\n",
    "                data = res.copy()\n",
    "                if save_path is not None:\n",
    "                    self.save_checkpoint(save_path, data)\n",
    "            if (i - last_improv) > patience:\n",
    "                break\n",
    "        return train_log\n",
    "    \n",
    "    def save_checkpoint(self, file_path, data):\n",
    "        default_ckpt = {\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "            \"optim_state\": self.optimizer.state_dict(),\n",
    "        }\n",
    "        assert \"state_dict\" not in data\n",
    "        assert \"optim_state\" not in data\n",
    "        default_ckpt.update(data)\n",
    "        torch.save(default_ckpt, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "from nlb_tools.make_tensors import make_train_input_tensors, make_eval_input_tensors\n",
    "from nlb_tools.evaluation import evaluate\n",
    "\n",
    "# Run parameters\n",
    "dataset_name = 'mc_maze_large'\n",
    "phase = 'val'\n",
    "bin_size = 5\n",
    "\n",
    "\n",
    "def prepareDatasets(dataset_name, phase='test', bin_size=5):\n",
    "    \"\"\"Function that extracts and formats data for training model\"\"\"\n",
    "    curr_path = os.getcwd()\n",
    "    fpath = curr_path + '/datasets/000138/sub-Jenkins/'\n",
    "    dataset = NWBDataset(fpath, \n",
    "        skip_fields=['cursor_pos', 'eye_pos', 'cursor_vel', 'eye_vel', 'hand_pos'])\n",
    "    dataset.resample(bin_size)\n",
    "    train_split = ['train', 'val'] if phase == 'test' else 'train'\n",
    "    eval_split = phase\n",
    "    train_dict = make_train_input_tensors(dataset, \\\n",
    "                                          dataset_name, \\\n",
    "                                          train_split, \\\n",
    "                                          save_file=False, \\\n",
    "                                          include_behavior=True, \\\n",
    "                                          include_forward_pred=True)\n",
    "    eval_dict = make_eval_input_tensors(dataset, dataset_name, eval_split, save_file=False)\n",
    "    del dataset\n",
    "\n",
    "    return train_dict, eval_dict\n",
    "    \n",
    "train_dict, eval_dict = prepareDatasets(dataset_name, phase, bin_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def prepareTrainData(train_dict, eval_dict):\n",
    "    training_input = train_dict['train_spikes_heldin'].copy()\n",
    "#     np.concatenate([\n",
    "#         train_dict['train_spikes_heldin'],\n",
    "#         np.zeros(train_dict['train_behavior'].shape),    \n",
    "#     ], axis=2)\n",
    "    training_output = train_dict['train_behavior'].copy()\n",
    "#     np.concatenate([\n",
    "#         train_dict['train_spikes_heldin'],\n",
    "#         train_dict['train_behavior'],\n",
    "#     ], axis=2),\n",
    "    eval_input = eval_dict['eval_spikes_heldin'].copy()\n",
    "    return training_input, training_output, eval_input\n",
    "\n",
    "\n",
    "        \n",
    "# Extract data\n",
    "training_input, training_output, eval_input = prepareTrainData(train_dict, eval_dict)\n",
    "\n",
    "# Train/val split and convert to Torch tensors\n",
    "num_train = int(round(training_input.shape[0] * 0.75))\n",
    "train_input = torch.Tensor(training_input[:num_train])\n",
    "train_output = torch.Tensor(training_output[:num_train])\n",
    "val_input = torch.Tensor(training_input[num_train:])\n",
    "val_output = torch.Tensor(training_output[num_train:])\n",
    "eval_input = torch.Tensor(eval_input)\n",
    "\n",
    "\n",
    "# Model hyperparameters\n",
    "DROPOUT = 0.46\n",
    "L2_WEIGHT = 5e-7\n",
    "LR_INIT = 1.5e-2\n",
    "CD_RATIO = 0.27\n",
    "HIDDEN_DIM = 40\n",
    "USE_GPU = False\n",
    "MAX_GPUS = 2\n",
    "\n",
    "\n",
    "#model_init, model_cfg, data, train_cfg, use_gpu=False, num_gpus=1\n",
    "# Train model\n",
    "runner = NLBRunner(\n",
    "    model_init=NLBRNN,\n",
    "    model_cfg={'input_dim': train_input.shape[2], 'hidden_dim': HIDDEN_DIM, 'output_dim': train_output.shape[2]},\n",
    "    data=(train_input, train_output, val_input, val_output, eval_input),\n",
    "    train_cfg={'lr': LR_INIT, 'alpha': L2_WEIGHT, 'cd_ratio': CD_RATIO},\n",
    "    use_gpu=USE_GPU,\n",
    "    num_gpus=MAX_GPUS,\n",
    ")\n",
    "\n",
    "#n_iter=1000, patience=200, save_path=None, verbose=False, log_frequency=50\n",
    "\n",
    "train_log = runner.train(n_iter=2000, patience=1000, save_path=None, verbose=True)\n",
    "\n",
    "\n",
    "## Save results\n",
    "#import pandas as pd\n",
    "#train_log = pd.DataFrame(train_log)\n",
    "#train_log.to_csv(os.path.join(model_dir, 'train_log.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Behavior (x, y velocities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "\n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "expected = train_output.data.numpy()[10]\n",
    "predicted = runner.model(train_input).data.numpy()[10]\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "zdata = np.linspace(0, train_output.shape[0])\n",
    "xdata = expected[:, 0].T #+ 0.1 * np.random.randn(100)\n",
    "ydata = expected[:, 1].T #+ 0.1 * np.random.randn(100)\n",
    "ax.scatter(xdata, ydata, zdata, c=zdata, cmap='Greens');\n",
    "# Data for three-dimensional scattered points\n",
    "# zdata = np.linspace(0, train_output.shape[0])\n",
    "xdata = predicted[:, 0].T #+ 0.1 * np.random.randn(100)\n",
    "ydata = predicted[:, 1].T #+ 0.1 * np.random.randn(100)\n",
    "ax.scatter(xdata, ydata, zdata, c=zdata, cmap='Red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Spike Raster Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEventData(inputData, threshold=0) :\n",
    "    num_neurons = 8\n",
    "    inp_np = inputData.data.numpy()\n",
    "    inp_np = np.where(inp_np > threshold, 1, 0)\n",
    "\n",
    "    a = np.nonzero(inp_np[0].T[:8])\n",
    "    neuralDataObj = {}\n",
    "    for n, time in zip(a[0], a[1]):\n",
    "        if n  in neuralDataObj:\n",
    "            neuralDataObj[n].append(time)\n",
    "        else :\n",
    "            neuralDataObj[n] = [time]\n",
    "    neuralData = []\n",
    "\n",
    "\n",
    "    for i in range(num_neurons) :\n",
    "        if i in neuralDataObj :\n",
    "            neuralData.append(neuralDataObj[i])\n",
    "        else :\n",
    "            neuralData.append([])\n",
    "\n",
    "    return neuralData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    " \n",
    "\n",
    "# Set the random seed for data generation\n",
    "\n",
    "np.random.seed(2)\n",
    "neuralData_input = getEventData(train_output)\n",
    "neuralData_output = getEventData(runner.model(train_input), threshold = 0.2)\n",
    "# Create rows of random data with 50 data points simulating rows of spike trains\n",
    "\n",
    "# neuralData = train_input[0].T[:8] \n",
    "# neuralData = np.random.random([8, 50]) + 3\n",
    "\n",
    "# Set different colors for each neuron\n",
    "\n",
    "colorCodes = np.array([[0, 0, 0],\n",
    "                        [1, 0, 0],\n",
    "                        [0, 1, 0],\n",
    "                      [0, 0, 1],\n",
    "                        [1, 1, 0],\n",
    "                        [1, 0, 1],\n",
    "                        [0, 1, 1],\n",
    "                        [1, 0, 1]])\n",
    "\n",
    "# Set spike colors for each neuron\n",
    "lineSize = [0.4, 0.3, 0.2, 0.8, 0.5, 0.6, 0.7, 0.9]                                  \n",
    "# Draw a spike raster plot\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "\n",
    "ax[0].eventplot(neuralData_input, color=colorCodes, linelengths = lineSize)     \n",
    "ax[1].eventplot(neuralData_output, color=colorCodes, linelengths = lineSize)     \n",
    "\n",
    "# Provide the title for the spike raster plot\n",
    "ax[0].set_title('Spike raster plot')\n",
    "# Give x axis label for the spike raster plot\n",
    "ax[1].set_xlabel('Neuron')\n",
    "# Give y axis label for the spike raster plot\n",
    "ax[0].set_ylabel('Spike')\n",
    "ax[1].set_ylabel('Spike')\n",
    "\n",
    "# Display the spike raster plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(dataset_name, phase='test', bin_size=5):\n",
    "    \"\"\"Function that extracts and formats data for training model\"\"\"\n",
    "    curr_path = os.getcwd()\n",
    "    fpath = curr_path + '/datasets/000138/sub-Jenkins/'\n",
    "    dataset = NWBDataset(fpath, \n",
    "        skip_fields=['cursor_pos', 'eye_pos', 'cursor_vel', 'eye_vel', 'hand_pos'])\n",
    "    dataset.resample(1)\n",
    "    train_split = ['train', 'val'] if phase == 'test' else 'train'\n",
    "    eval_split = phase\n",
    "    train_dict = make_train_input_tensors(dataset, dataset_name, train_split, save_file=False, include_forward_pred=True)\n",
    "    eval_dict = make_eval_input_tensors(dataset, dataset_name, eval_split, save_file=False)\n",
    "    training_input = np.concatenate([\n",
    "        train_dict['train_spikes_heldin'],\n",
    "        np.zeros(train_dict['train_spikes_heldin_forward'].shape),\n",
    "    ], axis=1)\n",
    "    training_output = np.concatenate([\n",
    "        np.concatenate([\n",
    "            train_dict['train_spikes_heldin'],\n",
    "            train_dict['train_spikes_heldin_forward'],\n",
    "        ], axis=1),\n",
    "        np.concatenate([\n",
    "            train_dict['train_spikes_heldout'],\n",
    "            train_dict['train_spikes_heldout_forward'],\n",
    "        ], axis=1),\n",
    "    ], axis=2)\n",
    "    eval_input = np.concatenate([\n",
    "        eval_dict['eval_spikes_heldin'],\n",
    "        np.zeros((\n",
    "            eval_dict['eval_spikes_heldin'].shape[0],\n",
    "            train_dict['train_spikes_heldin_forward'].shape[1],\n",
    "            eval_dict['eval_spikes_heldin'].shape[2]\n",
    "        )),\n",
    "    ], axis=1)\n",
    "    del dataset\n",
    "    return training_input, training_output, eval_input\n",
    "\n",
    "        \n",
    "# Run parameters\n",
    "dataset_name = 'mc_maze_large'\n",
    "phase = 'val'\n",
    "bin_size = 5\n",
    "\n",
    "# Extract data\n",
    "training_input, training_output, eval_input = get_data(dataset_name, phase, bin_size)\n",
    "# smoothen the spike data using gaussian filter of 5ms standaed deviation\n",
    "std_gaus = 5\n",
    "training_input_filt = scipy.ndimage.gaussian_filter1d(training_input, std_gaus, axis=1)\n",
    "training_output_filt = scipy.ndimage.gaussian_filter1d(np.float64(training_output), std_gaus, axis=1)\n",
    "eval_input_filt = scipy.ndimage.gaussian_filter1d(eval_input, std_gaus, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(training_input[1,:,0])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(training_input_filt[1,:,0])\n",
    "\n",
    "# Downsample all of the train and eval data to 5ms step size\n",
    "bin_size = 5\n",
    "resamp_int = np.arange(0, training_input_filt.shape[1], bin_size)\n",
    "training_input_filt = training_input_filt[:, resamp_int, :]\n",
    "training_output_filt = training_output_filt[:, resamp_int, :]\n",
    "eval_input_filt = eval_input_filt[:, resamp_int, :]\n",
    "#print(training_input.shape)\n",
    "#resamp_interval = np.arange(0, training_input.shape[1], 5)\n",
    "#print(training_input[:, resamp_interval,:].shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02b1bc2cb6aba3c63676f81fd881bdec751fcef939c531164eae59d8a44feb6a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
